{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import itertools as it\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = it.tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "def triplewise(iterable):\n",
    "    \"s -> (s0,s1,s2), (s1,s2,s3), (s2, s3,s4), ...\"\n",
    "    a, b = it.tee(iterable)\n",
    "    b, c = it.tee(b)\n",
    "    next(b, None)\n",
    "    next(c, None)\n",
    "    next(c, None)\n",
    "    return zip(a, b,c)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from functools import reduce, lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sample_chapters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def ne_preprocess(raw_text):\n",
    "    sents = nltk.sent_tokenize(raw_text)\n",
    "    tokenised_sents = [nltk.word_tokenize(sent) for sent in sents]\n",
    "    tagged_sents = nltk.pos_tag_sents(tokenised_sents)\n",
    "    ne_sents = nltk.ne_chunk_sents(tagged_sents, binary=True)\n",
    "    return list(ne_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_name(ne_tree):\n",
    "    return \" \".join([tagged_leaf[0] for tagged_leaf in ne_tree.leaves()])\n",
    "    \n",
    "def get_named_entities(sent):\n",
    "    for subsent in sent.subtrees(lambda ss:ss.label()=='NE'):\n",
    "        yield get_name(subsent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_main_character_by_voting(raw_text, breakdown=False, vote_over=get_named_entities, weights=it.repeat(1)):\n",
    "    ne_sents = ne_preprocess(raw_text)\n",
    "\n",
    "    votes=Counter()\n",
    "    for (name, weight) in zip(it.chain(*[vote_over(sent) for sent in ne_sents]), weights):\n",
    "        votes[name]+= weight\n",
    "        \n",
    "    if breakdown:\n",
    "        return votes\n",
    "    else:\n",
    "        if len(votes)>0:\n",
    "            return votes.most_common(1)[0][0]\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VERBS = frozenset({\"MD\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"})\n",
    "def patterned_name_entities(ne_sent):\n",
    "    for e1,e2 in pairwise(ne_sent):\n",
    "        if type(e1)!=nltk.tree.Tree or e1.label()!='NE':\n",
    "            continue\n",
    "        \n",
    "        if type(e2)==tuple and e2[1] in VERBS:\n",
    "            \n",
    "            yield get_name(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand\n",
      "Renarr\n",
      "Renarr\n",
      "Havaral\n",
      "Havaral\n",
      "Tug\n"
     ]
    }
   ],
   "source": [
    "print(get_main_character_by_voting(sample_memoriesoflight_rand,vote_over=patterned_name_entities))\n",
    "print(get_main_character_by_voting(sample_falloflight_renarr_1,vote_over=patterned_name_entities))\n",
    "print(get_main_character_by_voting(sample_falloflight_Renarr_2,vote_over=patterned_name_entities))\n",
    "print(get_main_character_by_voting(sample_falloflight_havaral_1,vote_over=patterned_name_entities))\n",
    "print(get_main_character_by_voting(sample_falloflight_havaral_2,vote_over=patterned_name_entities))\n",
    "print(get_main_character_by_voting(sample_falloflight_none,vote_over=patterned_name_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annotated_data = []\n",
    "\n",
    "def load_append(fn):    \n",
    "    with open(fn,\"r\") as fh:\n",
    "        annotated_data.extend(json.load(fh))\n",
    "        \n",
    "load_append(\"test_data/asoiaf01-04.json\")\n",
    "load_append(\"test_data/aDwD.json\")\n",
    "load_append(\"test_data/Rick Riordan - [Heroes of Olympus 02] - The Son of Neptune (epub).json\")\n",
    "load_append(\"test_data/Rick Riordan - [Heroes of Olympus 05] - The Blood of Olympus (epub).json\")\n",
    "load_append(\"test_data/Leigh Bardugo - [Dregs 01] - Six of Crows.json\")\n",
    "\n",
    "#load_append(\"test_data/Jonathan Stroud - [Bartimaeus 01] - The Amulet of Samarkand.json\")\n",
    "#load_append(\"test_data/Jonathan Stroud - [Bartimaeus 02] - The Golem's Eye.json\")\n",
    "#load_append(\"test_data/Jonathan Stroud - [Bartimaeus 03] - Ptolemy's Gate.json\")\n",
    "#load_append(\"test_data/Jonathan Stroud - [Bartimaeus 04] - The Ring of Solomon.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_texts=set(datum['text'] for datum in annotated_data)\n",
    "assert len(annotated_data)==len(unique_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reference_characters = [datum['character'] for datum in annotated_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reference_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_characters = [get_main_character_by_voting(datum['text'],\n",
    "                                                  vote_over=patterned_name_entities,\n",
    "                                                  weights=map(lambda x:1+1.0/x**0.5,it.count(1))\n",
    "                                                 ) for datum in annotated_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06277056277056277"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2nicknames = defaultdict(lambda:tuple(),{\n",
    "    \"Daenerys\": {\"Dany\",},\n",
    "    \"Eddard\" : {\"Ned\"},\n",
    "    \"Samwell\" :{\"Sam\"},\n",
    "})\n",
    "\n",
    "def correct(actual, ref):\n",
    "    return actual==ref or actual in name2nicknames[ref]   \n",
    "\n",
    "errors = [(index, actual, ref) for index, (actual, ref) in enumerate(zip(output_characters, reference_characters)) \n",
    "    if not (correct(actual,ref))]\n",
    "len(errors)/len(reference_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('S', [Tree('NE', [('Bill', 'NNP')]), ('was', 'VBD'), ('sad', 'JJ'), ('.', '.')]),\n",
       " Tree('S', [Tree('NE', [('Bill', 'NNP')]), ('hated', 'VBD'), Tree('NE', [('Joe', 'NNP')]), ('.', '.')])]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_preprocess(\"Bill was sad. Bill hated Joe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tag(item):\n",
    "    if type(item)==nltk.tree.Tree:\n",
    "        return item.label()\n",
    "    elif type(item)==tuple:\n",
    "        return item[1]\n",
    "    else:\n",
    "        assert(type(item) in [tuple, nltk.tree.Tree])\n",
    "\n",
    "from nltk.data import load\n",
    "tagset = list(load('help/tagsets/upenn_tagset.pickle').keys())\n",
    "tagset.append('NE')\n",
    "tagset.append('PAD')\n",
    "\n",
    "#FeatureVec = namedtuple(rank, count_below_top, lead, lag, verbs_before, verbs_after)\n",
    "#namedtuple(\"FeatureVec\", field_names =  \"rank, count_below_top, lead, lag, verbs_before, verbs_after\".split(\", \"))\n",
    "def FeatureVec():\n",
    "    vec=dict()\n",
    "    vec[\"occur_count\"]=0\n",
    "    vec[\"occur_percent\"]=0.0\n",
    "    vec[\"first_occur_position\"]=0\n",
    "    vec[\"first_occur_percent\"]=0.0\n",
    "    vec[\"last_occur_position\"]=0\n",
    "    vec[\"last_occur_percent\"]=0.0\n",
    "    vec[\"rank\"]=0\n",
    "    vec[\"rank_percent\"]=0\n",
    "    for tag in tagset:\n",
    "        vec[\"before_POS_was_\"+tag]=0\n",
    "        vec[\"after_POS_was_\"+tag]=0\n",
    "        vec[\"before_POS_was_percent_\"+tag]=0.0\n",
    "        vec[\"after_POS_was_percent_\"+tag]=0.0\n",
    "    return vec\n",
    "\n",
    "def get_feature_vectors(raw_text, best_n=10):\n",
    "    \n",
    "    ne_sents = ne_preprocess(raw_text)\n",
    "    ne_words = [(\"PAD\",\"PAD\")] +list(it.chain(*ne_sents)) + [(\"PAD\",\"PAD\")]\n",
    "    \n",
    "    feature_vecs = defaultdict(FeatureVec)\n",
    "    overall_counts = Counter()\n",
    "    \n",
    "    for ii, (before, cur, after) in enumerate(triplewise(ne_words)):\n",
    "        if type(cur)==nltk.tree.Tree and cur.label()=='NE':\n",
    "            name = get_name(cur)\n",
    "            overall_counts[name]+=1\n",
    "            vec = feature_vecs[name]\n",
    "            vec[\"occur_count\"]+=1 #should be equal to overall_counts\n",
    "            vec[\"before_POS_was_\"+get_tag(before)]+=1\n",
    "            vec[\"after_POS_was_\"+get_tag(after)]+=1\n",
    "            \n",
    "            vec[\"last_occur_position\"] = ii\n",
    "            if not(\"first_occur_position\" in vec):\n",
    "                vec[\"first_occur_position\"] = ii\n",
    "    \n",
    "    ###Basic data collected\n",
    "    number_named_entities = len(overall_counts)\n",
    "    \n",
    "    #Fill up to counts\n",
    "    while len(overall_counts)<best_n:\n",
    "        name = \"filler_\"+str(len(overall_counts))\n",
    "        feature_vecs[name]=FeatureVec()\n",
    "        overall_counts[name]=0\n",
    "        \n",
    "    #Final Percent processing, and flattening\n",
    "    vectors=[]\n",
    "    names=[]\n",
    "    vector_keys = list(FeatureVec().keys())\n",
    "    for rank,(name, count) in enumerate(overall_counts.most_common(n=best_n),1):\n",
    "        vec = feature_vecs[name]\n",
    "        assert(count==vec[\"occur_count\"])\n",
    "        vec[\"occur_percent\"] = 100*count/sum(overall_counts.values())\n",
    "        \n",
    "        vec[\"rank\"] = rank\n",
    "        vec[\"rank_percent\"] = 100*rank/number_named_entities\n",
    "        vec[\"first_occur_percent\"] = 100*vec[\"first_occur_position\"] / len(ne_words)\n",
    "        vec[\"last_occur_percent\"] = 100*vec[\"last_occur_position\"] / len(ne_words)\n",
    "        \n",
    "        for tag in tagset:\n",
    "            vec[\"before_POS_was_percent_\"+tag]=100*vec[\"before_POS_was_\"+tag]/count\n",
    "            vec[\"after_POS_was_percent_\"+tag]=100*vec[\"after_POS_was_\"+tag]/count\n",
    "        \n",
    "        \n",
    "        vectors.append(list(vec.values()))       \n",
    "        assert len(vectors[-1])==len(vector_keys), \"%i != %i\" % (len(vectors[-1]), len(vector_keys) )#Make sure I have everything\n",
    "        names.append(name)\n",
    "    \n",
    "    return names, vectors, vector_keys\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 196)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names, vectors, vector_keys = get_feature_vectors(sample_memoriesoflight_rand)\n",
    "X=np.asarray(vectors)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic Voting 0.08874458874458875\n",
    "[(3, 'Robert', 'Eddard'),\n",
    " (11, 'Robert', 'Eddard'),\n",
    " (38, 'Robert', 'Eddard'),\n",
    " (41, 'Bronn', 'Tyrion'),\n",
    " (62, 'Robb', 'Catelyn'),\n",
    " (104, 'Brienne', 'Catelyn'),\n",
    " (125, 'Cersei', 'Tyrion'),\n",
    " (127, 'Asha', 'Theon'),\n",
    " (143, 'Gendry', 'Arya'),\n",
    " (151, 'Brienne', 'Jaime'),\n",
    " (154, 'Robb', 'Catelyn'),\n",
    " (160, 'Robb', 'Catelyn'),\n",
    " (171, 'Brienne', 'Jaime'),\n",
    " (174, 'Hound', 'Arya'),\n",
    " (177, 'Brienne', 'Jaime'),\n",
    " (179, 'Lord Beric', 'Arya'),\n",
    " (185, 'Robb', 'Catelyn'),\n",
    " (189, 'Robb', 'Catelyn'),\n",
    " (193, 'Cersei', 'Tyrion'),\n",
    " (207, 'Kingsguard', 'Jaime'),\n",
    " (260, 'Illyrio', 'Tyrion'),\n",
    " (314, 'Percy', 'Hazel'),\n",
    " (324, 'Frank', 'Hazel'),\n",
    " (331, 'Phineas', 'Percy'),\n",
    " (342, 'Frank', 'Percy'),\n",
    " (353, 'Percy', 'Frank'),\n",
    " (380, 'Hylla', 'Reyna'),\n",
    " (385, 'Percy', 'Jason'),\n",
    " (387, 'Reyna', 'Nico'),\n",
    " (390, 'Apollo', 'Leo'),\n",
    " (391, 'Apollo', 'Leo'),\n",
    " (401, 'Jason', 'Piper'),\n",
    " (417, 'Kaz', 'Inej'),\n",
    " (419, 'Kaz', 'Inej'),\n",
    " (433, 'Jordie', 'Kaz'),\n",
    " (443, 'Nina', 'Inej'),\n",
    " (456, 'Nina', 'Matthias'),\n",
    " (457, 'Nina', 'Inej'),\n",
    " (458, 'Kuwei', 'Nina'),\n",
    " (460, 'Van Eck', 'Kaz'),\n",
    " (461, 'Rollins', 'Pekka')]\n",
    "\n",
    "\n",
    "#VERBs after Errors: 0.06493506493506493\n",
    "[(3, 'Robert', 'Eddard'),\n",
    " (11, 'Robert', 'Eddard'),\n",
    " (38, 'Robert', 'Eddard'),\n",
    " (41, 'Bronn', 'Tyrion'),\n",
    " (151, 'Brienne', 'Jaime'),\n",
    " (160, 'Robb', 'Catelyn'),\n",
    " (171, 'Qyburn', 'Jaime'),\n",
    " (174, 'Hound', 'Arya'),\n",
    " (177, 'Bolton', 'Jaime'),\n",
    " (193, 'Lord Tywin', 'Tyrion'),\n",
    " (205, 'Hound', 'Arya'),\n",
    " (314, 'Percy', 'Hazel'),\n",
    " (324, 'Frank', 'Hazel'),\n",
    " (331, 'Phineas', 'Percy'),\n",
    " (333, 'Phineas', 'Percy'),\n",
    " (342, 'Frank', 'Percy'),\n",
    " (353, 'Percy', 'Frank'),\n",
    " (380, 'Hylla', 'Reyna'),\n",
    " (385, 'Percy', 'Jason'),\n",
    " (387, 'Bryce', 'Nico'),\n",
    " (390, 'Apollo', 'Leo'),\n",
    " (393, 'Asclepius', 'Leo'),\n",
    " (401, 'Hazel', 'Piper'),\n",
    " (407, 'Zeus', 'Jason'),\n",
    " (419, 'Kaz', 'Inej'),\n",
    " (433, 'Jordie', 'Kaz'),\n",
    " (456, 'Nina', 'Matthias'),\n",
    " (457, 'Nina', 'Inej'),\n",
    " (458, 'Kuwei', 'Nina'),\n",
    " (461, 'Rollins', 'Pekka')]\n",
    "#\n",
    "\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Catelyn', 11),\n",
       " ('Ned', 7),\n",
       " ('Eyrie', 4),\n",
       " ('Riverrun', 4),\n",
       " ('Starks', 3),\n",
       " ('Worship', 2),\n",
       " ('Jon', 2),\n",
       " ('Robert', 2),\n",
       " ('Bran', 2),\n",
       " ('Winterfell', 2)]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_main_character_by_voting(annotated_data[1]['text'],True).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = list(ne_preprocess( annotated_data[1]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
