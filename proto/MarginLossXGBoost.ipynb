{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_int(char):\n",
    "    return int.from_bytes(char.encode(\"utf8\"),\"little\")\n",
    "\n",
    "def int_index(char):\n",
    "    return to_int(char)-to_int('A')\n",
    "\n",
    "def single_feature_extractor(word):\n",
    "    ret = np.zeros(int_index('z')+1)\n",
    "    for char in word:\n",
    "        ret[int_index(char)]+=1\n",
    "    return ret\n",
    "\n",
    "def feature_extractor(words):\n",
    "    return words, np.vstack([single_feature_extractor(ww) for ww in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Z', array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor(\"Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reference_characters=[\"bill\", \"tom\", \"tim\", \"dan\"]\n",
    "texts = [[ww] for ww in reference_characters]\n",
    "texts[0]+=['amy', 'sandy', 'danny']\n",
    "texts[1]+=['amy', 'sandy', 'sara']\n",
    "texts[2]+=['amy', 'sandy', 'sara', \"sally\"]\n",
    "texts[3]+=['amy', 'sandy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[slice(0, 4, None), slice(4, 8, None), slice(8, 13, None), slice(13, 16, None)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs = [] # Feature vectors\n",
    "Ys = [] # Binary as to if this feature is the target\n",
    "G_inds = [] # Group IDs\n",
    "last_ind = 0\n",
    "for (group,(reference_name, raw_text)) in enumerate(zip(reference_characters, texts)):\n",
    "    names, vectors = feature_extractor(raw_text)\n",
    "    Ys.extend([(name == reference_name) for name in names])\n",
    "    Xs.extend(vectors)\n",
    "    first_ind = last_ind\n",
    "    last_ind += len(names)\n",
    "    G_inds.append(slice(first_ind,last_ind))\n",
    "    \n",
    "Xs = np.asarray(Xs) \n",
    "Ys = np.asarray(Ys)\n",
    "    \n",
    "G_inds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://mathoverflow.net/a/51396/8800\n",
    "$$H_s(x) = \\begin{cases} \\tfrac{1}{2}-x & x \\le 0,\\\\\\\\\n",
    "\\tfrac{1}{2}(1-x)^2 & 0 < x < 1\\\\\\\\\n",
    "0 & x \\ge 1\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-baac0c0bdc19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x>=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=np.asarray([-1,1,1])*np.asarray([-1,0.99,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#objective(y_true, y_pred) -> grad, hess relative to each sample point\n",
    "def soft_hinge(y_true, y_pred):\n",
    "    y_true = 2*(y_true-0.5)\n",
    "    y_pred = 2*(y_pred-0.5)\n",
    "    \n",
    "    x=y_true*y_pred\n",
    "    loss= np.select([x<=0, x>=1], [0.5-x, 0], 0.5*(1-x)**2)\n",
    "    dLdYp = np.select([x<=0, x>=1], [-y_true, 0], -y_true*(1-x)) #-y_true+(y_true**2)y_pred\n",
    "    d2LdYp2 = np.select([x<=0, x>=1], [0, 0], -y_true**2)\n",
    "    \n",
    "    #loss = np.maximum(0, 1-y_true*y_pred)\n",
    "    #dLdYp = -y_pred # -y_true * (y_true*y_pred<1)\n",
    "    #d2LdYp2 = np.zeros_like(y_pred)\n",
    "    \n",
    "    print(\"y_true=\", y_true)\n",
    "    print(\"y_pred=\", y_pred)\n",
    "    print(\"loss=\",loss)\n",
    "    print(\"dLdYp=\",dLdYp)\n",
    "    print(\"-----------------\")\n",
    "    \n",
    "    return loss, dLdYp\n",
    "    \n",
    "def hinge(y_true, y_pred):\n",
    "    y_true = 2*(y_true-0.5)\n",
    "    y_pred = 2*(y_pred-0.5)\n",
    "    \n",
    "    loss = np.maximum(0, 1-y_true*y_pred)\n",
    "    dLdYp = -y_true * (y_true*y_pred<1)\n",
    "    d2LdYp2 = np.zeros_like(y_pred)\n",
    "    \n",
    "    grad = (y_true-y_pred)*loss\n",
    "    hess = -y_pred*dLdYp\n",
    "    \n",
    "    print(\"y_true=\", y_true)\n",
    "    print(\"y_pred=\", y_pred)\n",
    "    #print(\"loss=\",loss)\n",
    "    #print(\"dLdYp=\",dLdYp)\n",
    "    print(\"-----------------\")\n",
    "    \n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.46989524e-01,   1.00000000e+00],\n",
       "       [  2.28148332e-04,   0.00000000e+00],\n",
       "       [  2.28148332e-04,   0.00000000e+00],\n",
       "       [  2.28148332e-04,   0.00000000e+00],\n",
       "       [  6.46989524e-01,   1.00000000e+00],\n",
       "       [  2.28148332e-04,   0.00000000e+00],\n",
       "       [  2.28148332e-04,   0.00000000e+00],\n",
       "       [  6.46989524e-01,   0.00000000e+00],\n",
       "       [  6.46989524e-01,   1.00000000e+00],\n",
       "       [  2.28148332e-04,   0.00000000e+00],\n",
       "       [  2.28148332e-04,   0.00000000e+00],\n",
       "       [  6.46989524e-01,   0.00000000e+00],\n",
       "       [  2.28148332e-04,   0.00000000e+00],\n",
       "       [  6.46989524e-01,   1.00000000e+00],\n",
       "       [  2.28148332e-04,   0.00000000e+00],\n",
       "       [  2.28148332e-04,   0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relabel_grp_preds(y_true, y_pred):\n",
    "    assert (y_true==1).sum()==1\n",
    "    assert (y_true==0).sum()==len(y_true)-1\n",
    "    \n",
    "    true_inds = y_true>0.5 # Actually it is always 0 or 1, but force it to logical\n",
    "    other_inds = np.logical_not(true_inds)\n",
    "    \n",
    "    true_score = y_pred[true_inds]\n",
    "    mean_other_scores = y_pred[other_inds].mean()\n",
    "    \n",
    "    # Fake out the scores\n",
    "    y_pred[true_inds] = true_score - mean_other_scores/2\n",
    "    y_pred[other_inds] += true_score/(len(y_true))\n",
    "\n",
    "def var_logregobj(y_true, y_pred):\n",
    "    \n",
    "    #oy_pred=y_pred.copy()\n",
    "    for g_ind in G_inds:\n",
    "        relabel_grp_preds(y_true[g_ind], y_pred[g_ind]) #Inplace\n",
    "    \n",
    "    #print(np.vstack([oy_pred, y_pred, y_true]).T)\n",
    "    #print(\"------------\")\n",
    "    \n",
    "    y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "    grad = y_pred-y_true\n",
    "    hess = y_pred * (1.0 - y_true)\n",
    "    return grad, hess\n",
    "    \n",
    "def logregobj(y_true, y_pred):\n",
    "    y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "    grad = y_pred-y_true\n",
    "    hess = y_pred * (1.0 - y_true)\n",
    "    \n",
    "    #print(\"y_true=\", y_true)\n",
    "    #print(\"y_pred=\", y_pred)\n",
    "    #print(\"loss=\",grad)\n",
    "    #print(\"dLdYp=\",hess)\n",
    "    #print(\"-----------------\")\n",
    "    return grad, hess\n",
    "    \n",
    "cls = xgboost.XGBClassifier(silent=False, #objective=var_logregobj)\n",
    "                            objective=logregobj)\n",
    "\n",
    "cls.fit(Xs,Ys)\n",
    "np.vstack([cls.predict_proba(Xs)[:,1], Ys]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58630711,  1.        ],\n",
       "       [ 0.11137826,  0.        ],\n",
       "       [ 0.05736404,  0.        ],\n",
       "       [ 0.11137826,  0.        ],\n",
       "       [ 0.58630711,  1.        ],\n",
       "       [ 0.11137826,  0.        ],\n",
       "       [ 0.05736404,  0.        ],\n",
       "       [ 0.40762231,  0.        ],\n",
       "       [ 0.58630711,  1.        ],\n",
       "       [ 0.11137826,  0.        ],\n",
       "       [ 0.05736404,  0.        ],\n",
       "       [ 0.40762231,  0.        ],\n",
       "       [ 0.05736404,  0.        ],\n",
       "       [ 0.58630711,  1.        ],\n",
       "       [ 0.11137826,  0.        ],\n",
       "       [ 0.05736404,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = xgboost.XGBClassifier(silent=True)#,# objective=var_logregobj)\n",
    "                            #objective=logregobj)\n",
    "\n",
    "cls.fit(Xs,Ys)\n",
    "np.vstack([cls.predict_proba(Xs)[:,1], Ys]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xs [[ 0.4359949   0.02592623]\n",
      " [ 0.54966248  0.43532239]\n",
      " [ 0.4203678   0.33033482]\n",
      " [ 0.20464863  0.61927097]\n",
      " [ 0.29965467  0.26682728]]\n",
      "Ys [False  True  True False False]\n",
      "custom [-0.40514001 -0.40514001 -0.40514001 -0.40514001 -0.40514001]\n",
      "default [  4.96542430e-04   1.00032711e+00   9.98583794e-01   4.36608680e-04\n",
      "   2.93712132e-04]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "np.random.seed(2) #Reproducable training data\n",
    "Xs = np.random.rand(5,2)\n",
    "Ys = np.logical_and(Xs[:,1]>0.2,  Xs[:,0]>0.3)\n",
    "\n",
    "print(\"Xs\", Xs)\n",
    "print(\"Ys\", Ys)\n",
    "\n",
    "dtrain = xgb.DMatrix(Xs, Ys)\n",
    "\n",
    "# note: for customized objective function, we leave objective as default\n",
    "# note: what we are getting is margin value in prediction\n",
    "# you must know what you are doing\n",
    "param = {'max_depth': 2, 'eta': 1, 'silent': 1}\n",
    "\n",
    "# user define objective function, given prediction, return gradient and second order gradient\n",
    "# this is log likelihood loss\n",
    "def logregobj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = 1.0 / (1.0 + np.exp(-preds))\n",
    "    grad = preds - labels\n",
    "    hess = preds * (1.0 - preds)\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "# training with customized objective, we can also do step by step training\n",
    "# simply look at xgboost.py's implementation of train\n",
    "bst = xgb.train(param, dtrain, obj=logregobj)\n",
    "print(\"custom\", bst.predict(dtrain))\n",
    "dst = xgb.train(param, dtrain)\n",
    "print(\"default\", dst.predict(dtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.post3'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
