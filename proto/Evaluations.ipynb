{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import *\n",
    "\n",
    "import json\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from feature_extraction import *\n",
    "from classify import *\n",
    "from book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_WOT, ann_SOC, ann_ASOIAF, ann_SA\n",
      "lengths:  [432, 91, 256, 275]\n",
      "POVs:  52 9 15 6\n"
     ]
    }
   ],
   "source": [
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def all_metrics(tt,pp):\n",
    "    #prf = precision_recall_fscore_support(tt,pp, average='micro', labels=np.unique(tt))[0:3]\n",
    "    acc = accuracy_score(tt,pp)\n",
    "    return acc #np.hstack([prf, acc])\n",
    "\n",
    "all_metrics_names = [\"Acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classic_classifier():\n",
    "    return make_pipeline(\n",
    "    MaxAbsScaler(),\n",
    "    LogisticRegression(C=1, dual=False, penalty=\"l2\")\n",
    ")\n",
    "\n",
    "def make_highdim_classifier():\n",
    "    return make_pipeline(\n",
    "        StandardScaler(),\n",
    "        sklearn.svm.SVC(C=1.0, probability=True)\n",
    "    )\n",
    "\n",
    "\n",
    "CL_mdl = lambda: MLCharacterSolver(make_classic_classifier(), nicknames2name_comb)\n",
    "WE_mdl = lambda: MLCharacterSolver(make_highdim_classifier(), nicknames2name_comb, get_embedding_features)\n",
    "\n",
    "\n",
    "FM_mdl = lambda: FirstMentionedSolver(nicknames2name_comb)\n",
    "MC_mdl = lambda: MostMentionedSolver(nicknames2name_comb)\n",
    "\n",
    "datasets = [(\"WOT\", ann_WOT), (\"ASOIAF\", ann_ASOIAF), (\"SOC\", ann_SOC)]\n",
    "supdatasets = [(\"SA\", ann_SA)]\n",
    "base_mdls = [(\"ML Classical Features\", CL_mdl),\n",
    "             (\"ML Word Emb. Features\", WE_mdl),\n",
    "             (\"First Mentioned\", FM_mdl),\n",
    "             (\"Most Commonly Mentioned\", MC_mdl)\n",
    "       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## main eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ASOIAF', 'First Mentioned', '---')  0.25\n",
      "('ASOIAF', 'ML Classical Features', 'SOC')  0.953125\n",
      "('ASOIAF', 'ML Classical Features', 'WOT')  0.984375\n",
      "('ASOIAF', 'ML Word Emb. Features', 'SOC')  0.86328125\n",
      "('ASOIAF', 'ML Word Emb. Features', 'WOT')  0.9765625\n",
      "('ASOIAF', 'Most Commonly Mentioned', '---')  0.9140625\n",
      "('SOC', 'First Mentioned', '---')  0.42857142857142855\n",
      "('SOC', 'ML Classical Features', 'ASOIAF')  0.9230769230769231\n",
      "('SOC', 'ML Classical Features', 'WOT')  0.9230769230769231\n",
      "('SOC', 'ML Word Emb. Features', 'ASOIAF')  0.945054945054945\n",
      "('SOC', 'ML Word Emb. Features', 'WOT')  0.9340659340659341\n",
      "('SOC', 'Most Commonly Mentioned', '---')  0.7912087912087912\n",
      "('WOT', 'First Mentioned', '---')"
     ]
    }
   ],
   "source": [
    "def make_program(datasets, mdls):\n",
    "    program = OrderedDict()    \n",
    "    for (test_data_name, test_data),(mdl_name,mdl) in it.product(datasets, mdls):\n",
    "        if mdl_name[0:2]==\"ML\":\n",
    "            for (train_data_name, train_data) in datasets:\n",
    "                if train_data_name==test_data_name:\n",
    "                    continue\n",
    "                program[(test_data_name,mdl_name, train_data_name)] = (\n",
    "                    train_data,\n",
    "                    test_data,\n",
    "                    mdl()\n",
    "                )\n",
    "        else:\n",
    "            program[(test_data_name, mdl_name, \"---\")] = ([], test_data, mdl())\n",
    "    return program\n",
    "\n",
    "program = make_program(datasets, base_mdls)\n",
    "\n",
    "\n",
    "res = pd.DataFrame(index=pd.MultiIndex.from_tuples(program.keys()),\n",
    "                   columns = all_metrics_names)\n",
    "res.sort_index(inplace=True)\n",
    "\n",
    "for ind in res.index:\n",
    "    print(ind, end=\"\")\n",
    "    score = evaluate(*program[ind], metric=all_metrics)\n",
    "    res.loc[ind,:] = score\n",
    "    print(\" \", score)\n",
    "    res.to_csv(\"../results/maineval.csv\", index_label=[\"Test Set\", \"Method\", \"Train Set\"])\n",
    "    \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = MC_mdl()\n",
    "texts, ref_chars = extract_texts_and_characters(ann_WOT)\n",
    "output_chars = list(mdl.choose_characters(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_inds = np.asarray(ref_chars) != np.asarray(output_chars) == \"[No Characters Detected]\"\n",
    "\n",
    "list(zip(np.asarray(output_chars)[fail_inds], np.asarray(ref_chars)[fail_inds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(texts[np.asarray(output_chars) == \"[No Characters Detected]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Evaluation\n",
    "To test how much it effects things from different styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_program(datasets, mdls):\n",
    "    program = dict()    \n",
    "    for (data_name, data),(mdl_name,mdl) in it.product(datasets, mdls):\n",
    "        program[(data_name, mdl_name)] = (data, mdl())\n",
    "    return program\n",
    "program = make_program(datasets+[(\"Combined\", ann_comb)], base_mdls)\n",
    "\n",
    "\n",
    "res_xval = pd.DataFrame(index=pd.MultiIndex.from_tuples(program.keys()),\n",
    "                        columns = all_metrics_names)\n",
    "res_xval.sort_index(inplace=True)\n",
    "\n",
    "for ind in res_xval.index:\n",
    "    print(ind, end=\"\")\n",
    "    score = xval_evaluate(*program[ind], metric=all_metrics) \n",
    "    res_xval.loc[ind, :] = score\n",
    "    print(\" \", score)\n",
    "    res_xval.to_csv(\"../results/crosseval.csv\", index_label=[\"Dataset\", \"Method\"])\n",
    "    \n",
    "res_xval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supp data\n",
    "\n",
    "SA  ground truth is really weak.\n",
    "It is for a chapter which has maybe 4 scenses only 1-2 of which will actually be about that character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_program(datasets, supdatasets, mdls):\n",
    "    all_datasets = list(datasets)\n",
    "    all_datasets.extend(supdatasets)\n",
    "    \n",
    "    program = OrderedDict()    \n",
    "    for (test_data_name, test_data),(mdl_name,mdl) in it.product(datasets, mdls):\n",
    "        if mdl_name[0:2]==\"ML\":\n",
    "            combined_data = []\n",
    "            combined_data_names = []\n",
    "            for (train_data_name, train_data) in all_datasets:\n",
    "                if train_data_name==test_data_name:\n",
    "                    continue\n",
    "                combined_data.append(train_data)\n",
    "                combined_data_names.append(train_data_name)\n",
    "            \n",
    "            if len(combined_data) > 1:\n",
    "                train_data_name = \" and \".join(combined_data_names)\n",
    "                program[(test_data_name, mdl_name, train_data_name)] = (\n",
    "                    np.hstack(combined_data),\n",
    "                    test_data,\n",
    "                    mdl()\n",
    "                )\n",
    "    return program\n",
    "\n",
    "program = make_program(datasets,supdatasets, base_mdls)\n",
    "\n",
    "\n",
    "res = pd.DataFrame(index=pd.MultiIndex.from_tuples(program.keys()),\n",
    "                   columns = all_metrics_names)\n",
    "res.sort_index(inplace=True)\n",
    "\n",
    "for ind in res.index:\n",
    "    print(ind, end=\"\")\n",
    "    \n",
    "    score = evaluate(*program[ind], metric=all_metrics)\n",
    "    res.loc[ind,:] = score\n",
    "    print(\" \", score)\n",
    "    res.to_csv(\"../results/extradata.csv\", index_label=[\"Test Set\", \"Method\", \"Train Set\"])\n",
    "    \n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save some trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(ann, filename, model):\n",
    "    mdl = model()\n",
    "    mdl.train(*extract_texts_and_characters(ann))\n",
    "    joblib.dump(mdl, \"../trained_models/\"+filename+\".pkl\")\n",
    "    return mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_SOC = train_and_save_model(ann_SOC, \"CL_SOC\", CL_mdl)\n",
    "#HY_SOC = train_and_save_model(ann_SOC, \"HY_SOC\", HY_mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_ASOIAF = train_and_save_model(ann_ASOIAF, \"CL_ASOIAF\", CL_mdl)\n",
    "#HY_ASOIAF = train_and_save_model(ann_ASOIAF, \"HY_ASOIAF\", HY_mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature_importance(mdl):\n",
    "    _, _,vector_keys = get_feature_vectors(ann_comb[1]['text'])\n",
    "    feature_weights = list(zip(mdl.classifier.feature_importances_,vector_keys))\n",
    "    feature_weights.sort(reverse=True)\n",
    "    non_zero_weights = [(weight,name) for weight, name in feature_weights if weight>0]\n",
    "    print(\"Number of nonzeo weights: \", len(non_zero_weights))\n",
    "    print(\"\\n\".join(\", \".join(map(str,wt)) for wt in non_zero_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_importance(CL_SOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(HY_SOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(CL_ASOIAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(HY_ASOIAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
