{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import *\n",
    "\n",
    "import json\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from feature_extraction import *\n",
    "from classify import *\n",
    "from book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_WOT, ann_SOC, ann_ASOIAF, ann_SA\n",
      "lengths:  [432, 91, 256, 275]\n",
      "POVs:  52 9 15 6\n"
     ]
    }
   ],
   "source": [
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def all_metrics(tt,pp):\n",
    "    #prf = precision_recall_fscore_support(tt,pp, average='micro', labels=np.unique(tt))[0:3]\n",
    "    acc = accuracy_score(tt,pp)\n",
    "    return acc #np.hstack([prf, acc])\n",
    "\n",
    "all_metrics_names = [\"Acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classic_classifier():\n",
    "    return make_pipeline(\n",
    "    MaxAbsScaler(),\n",
    "    LogisticRegression(C=1, dual=False, penalty=\"l2\")\n",
    ")\n",
    "\n",
    "def make_highdim_classifier():\n",
    "    return make_pipeline(\n",
    "        StandardScaler(),\n",
    "        sklearn.svm.SVC(C=1.0, probability=True)\n",
    "    )\n",
    "\n",
    "\n",
    "CL_mdl = lambda: MLCharacterSolver(make_classic_classifier(), nicknames2name_comb)\n",
    "WE_mdl = lambda: MLCharacterSolver(make_highdim_classifier(), nicknames2name_comb, get_embedding_features)\n",
    "\n",
    "\n",
    "FM_mdl = lambda: FirstMentionedSolver(nicknames2name_comb)\n",
    "MC_mdl = lambda: MostMentionedSolver(nicknames2name_comb)\n",
    "\n",
    "datasets = [(\"WOT\", ann_WOT), (\"ASOIAF\", ann_ASOIAF), (\"SOC\", ann_SOC)]\n",
    "supdatasets = [(\"SA\", ann_SA)]\n",
    "base_mdls = [(\"ML Classical Features\", CL_mdl),\n",
    "             (\"ML Word Emb. Features\", WE_mdl),\n",
    "             (\"First Mentioned\", FM_mdl),\n",
    "             (\"Most Commonly Mentioned\", MC_mdl)\n",
    "       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## main eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ASOIAF', 'First Mentioned', '---')  0.25\n",
      "('ASOIAF', 'ML Classical Features', 'SOC')  0.953125\n",
      "('ASOIAF', 'ML Classical Features', 'WOT')  0.984375\n",
      "('ASOIAF', 'ML Word Emb. Features', 'SOC')  0.86328125\n",
      "('ASOIAF', 'ML Word Emb. Features', 'WOT')  0.9765625\n",
      "('ASOIAF', 'Most Commonly Mentioned', '---')  0.9140625\n",
      "('SOC', 'First Mentioned', '---')  0.42857142857142855\n",
      "('SOC', 'ML Classical Features', 'ASOIAF')  0.9230769230769231\n",
      "('SOC', 'ML Classical Features', 'WOT')  0.9230769230769231\n",
      "('SOC', 'ML Word Emb. Features', 'ASOIAF')  0.945054945054945\n",
      "('SOC', 'ML Word Emb. Features', 'WOT')  0.9340659340659341\n",
      "('SOC', 'Most Commonly Mentioned', '---')  0.7912087912087912\n",
      "('WOT', 'First Mentioned', '---')  0.04398148148148148\n",
      "('WOT', 'ML Classical Features', 'ASOIAF')  0.7453703703703703\n",
      "('WOT', 'ML Classical Features', 'SOC')  0.7013888888888888\n",
      "('WOT', 'ML Word Emb. Features', 'ASOIAF')  0.6990740740740741\n",
      "('WOT', 'ML Word Emb. Features', 'SOC')  0.5509259259259259\n",
      "('WOT', 'Most Commonly Mentioned', '---')  0.6597222222222222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">ASOIAF</th>\n",
       "      <th>First Mentioned</th>\n",
       "      <th>---</th>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ML Classical Features</th>\n",
       "      <th>SOC</th>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOT</th>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ML Word Emb. Features</th>\n",
       "      <th>SOC</th>\n",
       "      <td>0.863281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOT</th>\n",
       "      <td>0.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Commonly Mentioned</th>\n",
       "      <th>---</th>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">SOC</th>\n",
       "      <th>First Mentioned</th>\n",
       "      <th>---</th>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ML Classical Features</th>\n",
       "      <th>ASOIAF</th>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOT</th>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ML Word Emb. Features</th>\n",
       "      <th>ASOIAF</th>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOT</th>\n",
       "      <td>0.934066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Commonly Mentioned</th>\n",
       "      <th>---</th>\n",
       "      <td>0.791209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">WOT</th>\n",
       "      <th>First Mentioned</th>\n",
       "      <th>---</th>\n",
       "      <td>0.0439815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ML Classical Features</th>\n",
       "      <th>ASOIAF</th>\n",
       "      <td>0.74537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOC</th>\n",
       "      <td>0.701389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ML Word Emb. Features</th>\n",
       "      <th>ASOIAF</th>\n",
       "      <td>0.699074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOC</th>\n",
       "      <td>0.550926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Commonly Mentioned</th>\n",
       "      <th>---</th>\n",
       "      <td>0.659722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Acc\n",
       "ASOIAF First Mentioned         ---          0.25\n",
       "       ML Classical Features   SOC      0.953125\n",
       "                               WOT      0.984375\n",
       "       ML Word Emb. Features   SOC      0.863281\n",
       "                               WOT      0.976562\n",
       "       Most Commonly Mentioned ---      0.914062\n",
       "SOC    First Mentioned         ---      0.428571\n",
       "       ML Classical Features   ASOIAF   0.923077\n",
       "                               WOT      0.923077\n",
       "       ML Word Emb. Features   ASOIAF   0.945055\n",
       "                               WOT      0.934066\n",
       "       Most Commonly Mentioned ---      0.791209\n",
       "WOT    First Mentioned         ---     0.0439815\n",
       "       ML Classical Features   ASOIAF    0.74537\n",
       "                               SOC      0.701389\n",
       "       ML Word Emb. Features   ASOIAF   0.699074\n",
       "                               SOC      0.550926\n",
       "       Most Commonly Mentioned ---      0.659722"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_program(datasets, mdls):\n",
    "    program = OrderedDict()    \n",
    "    for (test_data_name, test_data),(mdl_name,mdl) in it.product(datasets, mdls):\n",
    "        if mdl_name[0:2]==\"ML\":\n",
    "            for (train_data_name, train_data) in datasets:\n",
    "                if train_data_name==test_data_name:\n",
    "                    continue\n",
    "                program[(test_data_name,mdl_name, train_data_name)] = (\n",
    "                    train_data,\n",
    "                    test_data,\n",
    "                    mdl()\n",
    "                )\n",
    "        else:\n",
    "            program[(test_data_name, mdl_name, \"---\")] = ([], test_data, mdl())\n",
    "    return program\n",
    "\n",
    "program = make_program(datasets, base_mdls)\n",
    "\n",
    "\n",
    "res = pd.DataFrame(index=pd.MultiIndex.from_tuples(program.keys()),\n",
    "                   columns = all_metrics_names)\n",
    "res.sort_index(inplace=True)\n",
    "\n",
    "for ind in res.index:\n",
    "    print(ind, end=\"\")\n",
    "    score = evaluate(*program[ind], metric=all_metrics)\n",
    "    res.loc[ind,:] = score\n",
    "    print(\" \", score)\n",
    "    res.to_csv(\"../resulthtos/maineval.csv\", index_label=[\"Test Set\", \"Method\", \"Train Set\"])\n",
    "    \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 2]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [1,2,3]\n",
    "np.random.shuffle(xs)\n",
    "xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ASOIAF', 'ML Classical Features', 'WOT+SOC')  0.9765625\n",
      "('ASOIAF', 'ML Word Emb. Features', 'WOT+SOC')  0.97265625\n",
      "('SOC', 'ML Classical Features', 'WOT+ASOIAF')  0.9340659340659341\n",
      "('SOC', 'ML Word Emb. Features', 'WOT+ASOIAF')"
     ]
    }
   ],
   "source": [
    "def make_program(datasets, mdls):\n",
    "    program = OrderedDict()    \n",
    "    for (test_data_name, test_data),(mdl_name,mdl) in it.product(datasets, mdls):\n",
    "        if mdl_name[0:2]!=\"ML\":\n",
    "            continue\n",
    "        full_train_data = []\n",
    "        full_train_data_names = []\n",
    "        for (train_data_name, train_data) in datasets:\n",
    "            if train_data_name==test_data_name:\n",
    "                continue\n",
    "            full_train_data_names.append(train_data_name)\n",
    "            full_train_data.extend(train_data)\n",
    "            \n",
    "        np.random.shuffle(full_train_data)\n",
    "        program[(test_data_name, mdl_name, \"+\".join(full_train_data_names))] = (\n",
    "            full_train_data,\n",
    "            test_data,\n",
    "            mdl()\n",
    "        )\n",
    "    return program\n",
    "\n",
    "program = make_program(datasets, base_mdls)\n",
    "\n",
    "\n",
    "res_comb = pd.DataFrame(index=pd.MultiIndex.from_tuples(program.keys()),\n",
    "                   columns = all_metrics_names)\n",
    "res_comb.sort_index(inplace=True)\n",
    "\n",
    "for ind in res_comb.index:\n",
    "    print(ind, end=\"\")\n",
    "    score = evaluate(*program[ind], metric=all_metrics)\n",
    "    res_comb.loc[ind,:] = score\n",
    "    print(\" \", score)\n",
    "    res_comb.to_csv(\"../results/combeval.csv\", index_label=[\"Test Set\", \"Method\", \"Train Set\"])\n",
    "    \n",
    "res_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Evaluation\n",
    "To test how much it effects things from different styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_program(datasets, mdls):\n",
    "    program = dict()    \n",
    "    for (data_name, data),(mdl_name,mdl) in it.product(datasets, mdls):\n",
    "        program[(data_name, mdl_name)] = (data, mdl())\n",
    "    return program\n",
    "program = make_program(datasets+[(\"Combined\", ann_comb)], base_mdls)\n",
    "\n",
    "\n",
    "res_xval = pd.DataFrame(index=pd.MultiIndex.from_tuples(program.keys()),\n",
    "                        columns = all_metrics_names)\n",
    "res_xval.sort_index(inplace=True)\n",
    "\n",
    "for ind in res_xval.index:\n",
    "    print(ind, end=\"\")\n",
    "    score = xval_evaluate(*program[ind], metric=all_metrics) \n",
    "    res_xval.loc[ind, :] = score\n",
    "    print(\" \", score)\n",
    "    res_xval.to_csv(\"../results/crosseval.csv\", index_label=[\"Dataset\", \"Method\"])\n",
    "    \n",
    "res_xval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supp data\n",
    "\n",
    "SA  ground truth is really weak.\n",
    "It is for a chapter which has maybe 4 scenses only 1-2 of which will actually be about that character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_program(datasets, supdatasets, mdls):\n",
    "    all_datasets = list(datasets)\n",
    "    all_datasets.extend(supdatasets)\n",
    "    \n",
    "    program = OrderedDict()    \n",
    "    for (test_data_name, test_data),(mdl_name,mdl) in it.product(datasets, mdls):\n",
    "        if mdl_name[0:2]==\"ML\":\n",
    "            combined_data = []\n",
    "            combined_data_names = []\n",
    "            for (train_data_name, train_data) in all_datasets:\n",
    "                if train_data_name==test_data_name:\n",
    "                    continue\n",
    "                combined_data.append(train_data)\n",
    "                combined_data_names.append(train_data_name)\n",
    "            \n",
    "            if len(combined_data) > 1:\n",
    "                train_data_name = \" and \".join(combined_data_names)\n",
    "                program[(test_data_name, mdl_name, train_data_name)] = (\n",
    "                    np.hstack(combined_data),\n",
    "                    test_data,\n",
    "                    mdl()\n",
    "                )\n",
    "    return program\n",
    "\n",
    "program = make_program(datasets,supdatasets, base_mdls)\n",
    "\n",
    "\n",
    "res = pd.DataFrame(index=pd.MultiIndex.from_tuples(program.keys()),\n",
    "                   columns = all_metrics_names)\n",
    "res.sort_index(inplace=True)\n",
    "\n",
    "for ind in res.index:\n",
    "    print(ind, end=\"\")\n",
    "    \n",
    "    score = evaluate(*program[ind], metric=all_metrics)\n",
    "    res.loc[ind,:] = score\n",
    "    print(\" \", score)\n",
    "    res.to_csv(\"../results/extradata.csv\", index_label=[\"Test Set\", \"Method\", \"Train Set\"])\n",
    "    \n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check WOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_CL_mdl = joblib.load(\"../trained_models/CL.pkl\")\n",
    "the_WE_mdl = joblib.load(\"../trained_models/WE.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, ref_chars = extract_texts_and_characters(ann_WOT)\n",
    "out_chars =  np.asarray(list(the_WE_mdl.choose_characters(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Moiraine', 'Rand'],\n",
       "       ['Mat', 'Rand'],\n",
       "       ['Hopper', 'Perrin'],\n",
       "       ['Min', 'Siuan'],\n",
       "       ['Mat', 'Rand'],\n",
       "       ['Isendre', 'Rand'],\n",
       "       ['Nynaeve', 'Rand'],\n",
       "       ['Bornhald', 'Rand'],\n",
       "       ['Min', 'Rand'],\n",
       "       ['Joline', 'Slayer'],\n",
       "       ['Perrin', 'Sevanna'],\n",
       "       ['Sorilea', 'Sulin'],\n",
       "       ['Elayne', 'Nynaeve'],\n",
       "       ['Shiaine', 'Mili'],\n",
       "       ['Egeanin', 'Mat'],\n",
       "       ['Domon', 'Rand'],\n",
       "       ['Egwene', 'Nynaeve'],\n",
       "       ['Nynaeve', 'Reanne'],\n",
       "       ['Nynaeve', 'Rand'],\n",
       "       ['Min', 'Thom'],\n",
       "       ['Mat', 'Rand'],\n",
       "       ['Elayne', 'Moghedien'],\n",
       "       ['Mat', 'Rand'],\n",
       "       ['Egwene', 'Mat'],\n",
       "       ['Moiraine', 'Rand'],\n",
       "       ['Rand', 'Verin'],\n",
       "       ['Faile', 'Perrin'],\n",
       "       ['Alanna', 'Egwene'],\n",
       "       ['Gaul', 'Perrin'],\n",
       "       ['Tar Valon', 'Mat'],\n",
       "       ['Elayne', 'Perrin'],\n",
       "       ['Aviendha', 'Rand'],\n",
       "       ['Min', 'Jaret'],\n",
       "       ['Nynaeve', 'Rand'],\n",
       "       ['Bryne', 'Gareth'],\n",
       "       ['Rand', 'Thom'],\n",
       "       ['Domon', 'Geofram'],\n",
       "       ['Alviarin', 'Seaine'],\n",
       "       ['Perrin', 'Rand'],\n",
       "       ['Egwene', 'Rand'],\n",
       "       ['Nynaeve', 'Rand'],\n",
       "       ['Bornhald', 'Perrin'],\n",
       "       ['Min', 'Morgase'],\n",
       "       ['Elaida', 'Suroth'],\n",
       "       ['Falion', 'Noal'],\n",
       "       ['Egwene', 'Moghedien'],\n",
       "       ['Morgase', 'Padan'],\n",
       "       ['Jac', 'Perrin'],\n",
       "       ['Silviana', 'Elaida'],\n",
       "       ['Cadsuane', 'Rand'],\n",
       "       ['Master Gill', 'Rand'],\n",
       "       ['Egwene', 'Sheriam'],\n",
       "       ['Nynaeve', 'Egwene'],\n",
       "       ['Fain', 'Thom'],\n",
       "       ['Kennar Miraj', 'Varek'],\n",
       "       ['Egwene', 'Nesune'],\n",
       "       ['Egwene', 'Min'],\n",
       "       ['Min', 'Gareth'],\n",
       "       ['Perrin', 'Sorilea'],\n",
       "       ['Egeanin', 'Jaichim'],\n",
       "       ['Balwer', 'Pedron'],\n",
       "       ['Bayle', 'Egeanin'],\n",
       "       ['Mat', 'Reanne'],\n",
       "       ['Rand', 'Raefar'],\n",
       "       ['Elayne', 'Nynaeve'],\n",
       "       ['Lewin', 'Rand'],\n",
       "       ['Cadsuane', 'Demandred'],\n",
       "       ['Elayne', 'Sheriam'],\n",
       "       ['Selucia', 'Tuon'],\n",
       "       ['Segan', 'Egwene'],\n",
       "       ['Egwene', 'Nynaeve'],\n",
       "       ['Elayne', 'Min'],\n",
       "       ['Liandrin', 'Padan'],\n",
       "       ['Egwene', 'Moiraine'],\n",
       "       ['Sheriam', 'Egwene'],\n",
       "       ['Nynaeve', 'Rand'],\n",
       "       ['Min', 'Rand'],\n",
       "       ['Nynaeve', 'Mat'],\n",
       "       ['Mat', 'Rand'],\n",
       "       ['Elayne', 'Siuan'],\n",
       "       ['Sulin', 'Rand'],\n",
       "       ['Perrin', 'Rand'],\n",
       "       ['Perrin', 'Rand'],\n",
       "       ['Egwene', 'Siuan'],\n",
       "       ['Elayne', 'Nynaeve'],\n",
       "       ['Galina', 'Shaidar'],\n",
       "       ['Perrin', 'Rand'],\n",
       "       ['Juilin', 'Mat'],\n",
       "       ['Elayne', 'Egeanin'],\n",
       "       ['Valda', 'Rhadam'],\n",
       "       ['Elayne', 'Myrelle'],\n",
       "       ['Elayne', 'Moridin'],\n",
       "       ['Egwene', 'Sarene'],\n",
       "       ['Barmellin', 'Verin'],\n",
       "       ['Perrin', 'Rand']], dtype='<U12')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(list(zip(out_chars, ref_chars)))[out_chars != ref_chars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set accurasy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ASOIAF', 'ML Classical Features', 'ASOIAF')  0.98046875\n",
      "('ASOIAF', 'ML Word Emb. Features', 'ASOIAF')  0.98828125\n",
      "('SOC', 'ML Classical Features', 'SOC')  0.945054945054945\n",
      "('SOC', 'ML Word Emb. Features', 'SOC')  0.9560439560439561\n",
      "('WOT', 'ML Classical Features', 'WOT')  0.7847222222222222\n",
      "('WOT', 'ML Word Emb. Features', 'WOT')  0.7939814814814815\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ASOIAF</th>\n",
       "      <th>ML Classical Features</th>\n",
       "      <th>ASOIAF</th>\n",
       "      <td>0.980469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML Word Emb. Features</th>\n",
       "      <th>ASOIAF</th>\n",
       "      <td>0.988281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SOC</th>\n",
       "      <th>ML Classical Features</th>\n",
       "      <th>SOC</th>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML Word Emb. Features</th>\n",
       "      <th>SOC</th>\n",
       "      <td>0.956044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">WOT</th>\n",
       "      <th>ML Classical Features</th>\n",
       "      <th>WOT</th>\n",
       "      <td>0.784722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML Word Emb. Features</th>\n",
       "      <th>WOT</th>\n",
       "      <td>0.793981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Acc\n",
       "ASOIAF ML Classical Features ASOIAF  0.980469\n",
       "       ML Word Emb. Features ASOIAF  0.988281\n",
       "SOC    ML Classical Features SOC     0.945055\n",
       "       ML Word Emb. Features SOC     0.956044\n",
       "WOT    ML Classical Features WOT     0.784722\n",
       "       ML Word Emb. Features WOT     0.793981"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_program(datasets, mdls):\n",
    "    program = OrderedDict()    \n",
    "    for (test_data_name, test_data),(mdl_name,mdl) in it.product(datasets, mdls):\n",
    "        if mdl_name[0:2]!=\"ML\":\n",
    "            continue\n",
    "        program[(test_data_name,mdl_name, test_data_name)] = (\n",
    "            test_data,\n",
    "            test_data,\n",
    "            mdl()\n",
    "        )\n",
    "    return program\n",
    "\n",
    "program = make_program(datasets, base_mdls)\n",
    "\n",
    "res_train = pd.DataFrame(index=pd.MultiIndex.from_tuples(program.keys()),\n",
    "                   columns = all_metrics_names)\n",
    "res_train.sort_index(inplace=True)\n",
    "\n",
    "for ind in res_train.index:\n",
    "    print(ind, end=\"\")\n",
    "    score = evaluate(*program[ind], metric=all_metrics)\n",
    "    res_train.loc[ind,:] = score\n",
    "    print(\" \", score)\n",
    "    res_train.to_csv(\"../results/traineval.csv\", index_label=[\"Test Set\", \"Method\", \"Train Set\"])\n",
    "    \n",
    "res_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature_importance(mdl):\n",
    "    _, _,vector_keys = get_feature_vectors(ann_comb[1]['text'])\n",
    "    feature_weights = list(zip(mdl.classifier.feature_importances_,vector_keys))\n",
    "    feature_weights.sort(reverse=True)\n",
    "    non_zero_weights = [(weight,name) for weight, name in feature_weights if weight>0]\n",
    "    print(\"Number of nonzeo weights: \", len(non_zero_weights))\n",
    "    print(\"\\n\".join(\", \".join(map(str,wt)) for wt in non_zero_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_importance(CL_SOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(HY_SOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(CL_ASOIAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(HY_ASOIAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
