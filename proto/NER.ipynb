{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import json\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn.tree\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sample_chapters import *\n",
    "from feature_extraction import *\n",
    "from classify import *\n",
    "from book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_comb, ann_SOC, ann_ASOIAF, ann_SA\n",
      "lengths:  [622, 91, 256, 275]\n",
      "POVs:  30 9 15 6\n"
     ]
    }
   ],
   "source": [
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Grouping Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relabel_grp_preds(y_true, y_pred):\n",
    "    if  y_true.sum()<1:\n",
    "        return #No positive samples, thus no rescaling required\n",
    "    \n",
    "    assert (y_true==0).sum()==len(y_true)-1\n",
    "\n",
    "    target_inds = y_true>0.5 # Actually it is always 0 or 1, but force it to logical\n",
    "    other_inds = np.logical_not(target_inds)\n",
    "\n",
    "    target_score = y_pred[target_inds]\n",
    "    target_penalty = 0 #y_pred[other_inds].max()\n",
    "    other_penalty = 0#target_score/(len(y_true)-1)\n",
    "\n",
    "    # Fake out the scores\n",
    "    y_pred[target_inds] -= target_penalty # increase it's loss\n",
    "    y_pred[other_inds]  += other_penalty #Decreases their loss\n",
    "\n",
    "class GroupedMLCharacterSolver(MLCharacterSolver):\n",
    "\n",
    "\n",
    "    def var_logregobj(y_true, y_pred):\n",
    "\n",
    "        #oy_pred=y_pred.copy()\n",
    "        for g_ind in G_inds:\n",
    "            relabel_grp_preds(y_true[g_ind], y_pred[g_ind]) #Inplace\n",
    "\n",
    "        #print(np.vstack([oy_pred, y_pred, y_true]).T)\n",
    "        #print(\"------------\")\n",
    "\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "        grad = y_pred-y_true\n",
    "        hess = y_pred * (1.0 - y_true)\n",
    "        return grad, hess\n",
    "\n",
    "    def train(self, texts, reference_characters):\n",
    "        Xs = [] # Feature vectors\n",
    "        Ys = [] # Binary as to if this feature is the target\n",
    "        last_ind = 0\n",
    "        G_inds = []\n",
    "        for reference_name, raw_text in zip(reference_characters, texts):\n",
    "            names, vectors, _ = self.feature_extractor(raw_text)\n",
    "            Xs.extend(vectors)\n",
    "            y = [(name == reference_name) for name in names]\n",
    "            Ys.extend(y)\n",
    "            first_ind = last_ind\n",
    "            last_ind += len(names)\n",
    "            G_inds.append(slice(first_ind,last_ind))\n",
    "\n",
    "        Xs = np.asarray(Xs)\n",
    "        Ys = np.asarray(Ys)\n",
    "        assert Xs.shape[0]==Ys.shape[0], (Xs.shape[0], Ys.shape[0])\n",
    "        assert len(Xs.shape)==2, \"Xs.shape = \"+str(Xs.shape)\n",
    "        assert Xs.shape[1]>2, \"Xs.shape[1] = \"+str(Xs.shape[1])\n",
    "\n",
    "        # closure over G_inds\n",
    "        def var_logregobj(y_true, y_pred):\n",
    "            #for g_ind in G_inds:\n",
    "            #    relabel_grp_preds(y_true[g_ind], y_pred[g_ind]) #Inplace\n",
    "            \n",
    "            y_pred =  1.0 / (1.0 + np.exp(-y_pred))\n",
    "            grad = y_pred-y_true\n",
    "            eps=1e-16;\n",
    "            hess = np.maximum(y_pred * (1.0 - y_pred), eps);\n",
    "            return grad, hess\n",
    "\n",
    "        self.classifier.objective = var_logregobj #Over write it to the closure\n",
    "        self.classifier.fit(Xs,Ys)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ann_SOC_texts, ann_SOC_chars  = extract_texts_and_characters(ann_SOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clmdd = CL_mdl()\n",
    "evaluate(ann_SOC, ann_SOC, clmdd)\n",
    "clmdd.choose_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GML_mdl = GroupedMLCharacterSolver(XGBClassifier(), nicknames2name=nicknames2name_comb)\n",
    "print(evaluate(ann_SOC, ann_SOC, GML_mdl))\n",
    "out_chars = list(GML_mdl.choose_characters(ann_SOC_texts) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inds = ann_SOC_chars!=out_chars\n",
    "hard_chars = ann_SOC_chars[inds]\n",
    "hard_texts = ann_SOC_texts[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(zip(*clmdd.character_scores(hard_texts[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(zip(*GML_mdl.character_scores(hard_texts[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hard_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GML_mdl = GroupedMLCharacterSolver(XGBClassifier(), nicknames2name=nicknames2name_comb)\n",
    "evaluate(ann_ASOIAF, ann_SOC, GML_mdl) #0.86956521739130432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate(ann_ASOIAF, ann_SOC, CL_mdl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out aumented Word Emb + Occurrencee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WO_mdl = lambda: MLCharacterSolver(\n",
    "    XGBClassifier(),\n",
    "    nicknames2name_comb,\n",
    "    lambda x: get_embedding_features(x, include_occur_count_statistics=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate(ann_ASOIAF, ann_SOC, WO_mdl(), metric=all_metrics) #0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate(ann_SOC, ann_ASOIAF, WO_mdl(), metric=all_metrics) #0.921"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More rank features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1), (3, 1), (2, 1), (1, 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([1,1,1,2,3,4]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_rank_feature_vectors(raw_text):\n",
    "    ne_words = 2*[PADDING_TOKEN] + ne_preprocess(raw_text) + 2*[PADDING_TOKEN]\n",
    "    features = [\"occurs\"]\n",
    "    features.extend([\"POS_w-1_\"+tag for tag in tagset])\n",
    "    features.extend([\"POS_w+1_\"+tag for tag in tagset])\n",
    "    \n",
    "    feature_counters = OrderedDict((feature, Counter()) for feature in features)\n",
    "    all_names = set()\n",
    "    for ii, (before, cur, after) in enumerate(triplewise(ne_words)):\n",
    "        if type(cur)==nltk.tree.Tree and cur.label()=='NE':\n",
    "            name = get_name(cur)\n",
    "            all_names.add(name)\n",
    "            feature_counters[\"occurs\"][name]+=1\n",
    "            feature_counters[\"POS_w-1_\" + get_tag(before)][name]+=1\n",
    "            feature_counters[\"POS_w+1_\" + get_tag(after)][name]+=1\n",
    "                \n",
    "    #Final Percent processing, and flattening\n",
    "    name2vecs = defaultdict(lambda: np.zeros(2*len(features)))\n",
    "    for feature_ii_base, feature_counts in enumerate(feature_counters.values()):\n",
    "        feature_ii = 2*feature_ii_base\n",
    "        for inv_rank,(name, count) in enumerate(feature_counts.most_common()[::-1],1):\n",
    "            # note: inv_rank, so higher is better, so zero is sane default\n",
    "            # make it out of 1 so has upper and lower bound\n",
    "            name2vecs[name][feature_ii] = inv_rank/len(all_names)\n",
    "            name2vecs[name][feature_ii+1] = count\n",
    "            \n",
    "    \n",
    "    return list(name2vecs.keys()), list(name2vecs.values()), features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86813186813186816"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_mdl = MLCharacterSolver(\n",
    "    XGBClassifier(),\n",
    "    nicknames2name=nicknames2name_comb,\n",
    "    feature_extractor= get_rank_feature_vectors\n",
    ")\n",
    "evaluate(ann_ASOIAF, ann_SOC, ML_mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More POS features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def ExpandedFeatureVec(half_win):\n",
    "    vec=OrderedDict()\n",
    "    vec[\"occur_count\"]=0\n",
    "    vec[\"occur_percent\"]=0.0\n",
    "    vec[\"first_occur_position\"]=0\n",
    "    vec[\"first_occur_percent\"]=0.0\n",
    "    vec[\"last_occur_position\"]=0\n",
    "    vec[\"last_occur_percent\"]=0.0\n",
    "    vec[\"occur_rank\"]=0\n",
    "    vec[\"occur_rank_percent\"]=0\n",
    "    for tag in tagset:\n",
    "        for ii in range(1,half_win+1):\n",
    "            vec[\"POS_w-%i_\"%ii +tag]=0\n",
    "            vec[\"POS_w+%i_\"%ii +tag]=0\n",
    "            vec[\"POS_w-%i__was_percent_\"%ii +tag]=0.0\n",
    "            vec[\"POS_w+%i__was_percent_\"%ii +tag]=0.0\n",
    "        \n",
    "    return vec\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns a list of names, feature_vectors, and a definition of the feature vector keys\n",
    "\"\"\"\n",
    "def get_more_feature_vectors(raw_text):\n",
    "    \n",
    "    ne_words = 2*[PADDING_TOKEN] + ne_preprocess(raw_text) + 2*[PADDING_TOKEN]\n",
    "    \n",
    "    feature_vecs = defaultdict(lambda: ExpandedFeatureVec(2))\n",
    "    overall_counts = Counter()\n",
    "    \n",
    "    for ii, (wn2,wn1,cur,wp1,wp2) in enumerate(nwise(5, ne_words)):\n",
    "        if type(cur)==nltk.tree.Tree and cur.label()=='NE':\n",
    "            name = get_name(cur)\n",
    "            \n",
    "            overall_counts[name]+=1\n",
    "            vec = feature_vecs[name]\n",
    "            vec[\"occur_count\"]+=1 #should be equal to overall_counts\n",
    "\n",
    "            vec[\"POS_w-2_\" +get_tag(wn2)]+=1\n",
    "            vec[\"POS_w-1_\" +get_tag(wn1)]+=1\n",
    "            vec[\"POS_w+1_\" +get_tag(wp1)]+=1\n",
    "            vec[\"POS_w+2_\" +get_tag(wp2)]+=1\n",
    "            \n",
    "            vec[\"last_occur_position\"] = ii #update last seen to this sent\n",
    "            if not(\"first_occur_position\" in vec): #if not set then this sent must be first time\n",
    "                vec[\"first_occur_position\"] = ii\n",
    "    \n",
    "    ###Basic data collected\n",
    "    number_named_entities = len(overall_counts)\n",
    "\n",
    "    #Final Percent processing, and flattening\n",
    "    vectors=[]\n",
    "    names=[]\n",
    "    vector_keys = list(ExpandedFeatureVec(2).keys())\n",
    "    for rank,(name, count) in enumerate(overall_counts.most_common(),1):\n",
    "        \n",
    "        vec = feature_vecs[name]\n",
    "        assert(count==vec[\"occur_count\"])\n",
    "        vec[\"occur_percent\"] = 100*count/sum(overall_counts.values())\n",
    "        \n",
    "        vec[\"occur_rank\"] = rank\n",
    "        vec[\"occur_rank_percent\"] = 100*rank/number_named_entities\n",
    "        vec[\"first_occur_percent\"] = 100*vec[\"first_occur_position\"] / len(ne_words)\n",
    "        vec[\"last_occur_percent\"] = 100*vec[\"last_occur_position\"] / len(ne_words)\n",
    "        \n",
    "        for tag in tagset:\n",
    "            vec[\"POS_w-2__was_percent_\" + tag]=vec[\"POS_w-2_\"+tag]*100/count\n",
    "            vec[\"POS_w-1__was_percent_\" + tag]=vec[\"POS_w-1_\"+tag]*100/count\n",
    "            vec[\"POS_w+1__was_percent_\" + tag]=vec[\"POS_w+1_\"+tag]*100/count\n",
    "            vec[\"POS_w+2__was_percent_\" + tag]=vec[\"POS_w+2_\"+tag]*100/count\n",
    "        \n",
    "        \n",
    "        vectors.append(list(vec.values()))\n",
    "        assert len(vectors[-1])==len(vector_keys), \"%i != %i\" % (len(vectors[-1]), len(vector_keys) )#Make sure I have everything\n",
    "        names.append(name)\n",
    "    \n",
    "    return names, np.asarray(vectors), vector_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84615384615384615"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_mdl = MLCharacterSolver(\n",
    "    XGBClassifier(),\n",
    "    nicknames2name=nicknames2name_comb,\n",
    "    feature_extractor= get_more_feature_vectors\n",
    ")\n",
    "evaluate(ann_ASOIAF, ann_SOC, ML_mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swap NLP Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GoldCorpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c080f6458562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcli_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexplain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/spacy/cli/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpackage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/spacy/cli/train.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoldCorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprints\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'GoldCorpus'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
