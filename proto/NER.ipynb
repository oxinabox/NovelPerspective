{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import json\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn.tree\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sample_chapters import *\n",
    "from feature_extraction import *\n",
    "from classify import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts_and_characters(annotated_data):\n",
    "    full_characters = np.asarray([datum['character'] for datum in annotated_data])\n",
    "    full_texts = np.asarray([datum['text'] for datum in annotated_data])\n",
    "    return full_texts, full_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validation_evaluate(annotated_data, solver, n_splits=10, metric=accuracy_score):\n",
    "    \n",
    "    full_texts, full_characters = extract_texts_and_characters(annotated_data)\n",
    "    \n",
    "    scores = []\n",
    "    for train_inds, test_inds in KFold(n_splits=n_splits).split(annotated_data):\n",
    "        train_texts = full_texts[train_inds]\n",
    "        train_characters = full_characters[train_inds]\n",
    "        \n",
    "        test_characters = full_characters[test_inds]\n",
    "        test_texts = full_texts[test_inds]\n",
    "        \n",
    "\n",
    "        solver.train(train_texts, train_characters)\n",
    "        score = solver.test(test_texts, test_characters, metric=metric)\n",
    "        \n",
    "        print(score)\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nicknames2name_comb = {\n",
    "    \"Dany\":\"Daenerys\",\n",
    "    \"Ned\" : \"Eddard\",\n",
    "    \"Sam\" : \"Samwell\",\n",
    "    \"Rollins\" : \"Pekka\"\n",
    "}\n",
    "\n",
    "with open(\"../flat_data/asoif01-04.json\",\"r\") as fh:\n",
    "    ann_GoT = np.asarray(json.load(fh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8666666666666667, 0.8888888888888888, 0.8765432098765432)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7333333333333333, 0.8, 0.76)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8611111111111112, 0.8888888888888888, 0.873015873015873)\n",
      "(1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7772727272727273, 0.8181818181818182, 0.7950937950937952)\n",
      "(1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6666666666666666, 0.7692307692307693, 0.7025641025641025)\n",
      "(1.0, 1.0, 1.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "(1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9025819283597062"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validation_evaluate(ann_GoT, MLCharacterSolver(XGBClassifier(), nicknames2name_comb),\n",
    "                                  metric=lambda tt,pp: precision_recall_fscore_support(tt,pp, average='macro')[0:3])\n",
    "np.mean(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89050505, 0.91651904, 0.9007217 ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.09375, 0.16666666666666666, 0.11071428571428571)\n",
      "(0.08333333333333333, 0.14285714285714285, 0.09999999999999999)\n",
      "(0.04285714285714286, 0.10714285714285714, 0.058333333333333334)\n",
      "(0.07976190476190477, 0.21428571428571427, 0.11394557823129252)\n",
      "(0.07183908045977012, 0.13793103448275862, 0.08850574712643677)\n",
      "(0.06055555555555556, 0.13333333333333333, 0.08126984126984128)\n",
      "(0.1358974358974359, 0.23076923076923078, 0.16025641025641027)\n",
      "(0.0734567901234568, 0.16666666666666666, 0.09770723104056438)\n",
      "(0.18939393939393942, 0.29545454545454547, 0.22121212121212122)\n",
      "(0.03518518518518518, 0.1111111111111111, 0.05185185185185185)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.08660304, 0.17062183, 0.10837964])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validation_evaluate(ann_GoT, FirstMentionedSolver(nicknames2name_comb),\n",
    "                                  metric=lambda tt,pp: precision_recall_fscore_support(tt,pp, average='macro')[0:3])\n",
    "np.mean(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9019230769230769, 0.9230769230769231, 0.9065934065934067)\n",
      "(0.6965811965811965, 0.8076923076923077, 0.7373626373626373)\n",
      "(0.8846153846153846, 0.9230769230769231, 0.8974358974358974)\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.8057692307692308, 0.8846153846153846, 0.8382173382173382)\n",
      "(0.8782051282051282, 0.9230769230769231, 0.8948717948717948)\n",
      "(0.6746666666666666, 0.8, 0.7235555555555555)\n",
      "(0.8266666666666667, 0.88, 0.8440000000000001)\n",
      "(0.9333333333333332, 0.96, 0.9440000000000001)\n",
      "(1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.86017607, 0.91015385, 0.87860366])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validation_evaluate(ann_GoT, MostMentionedSolver(nicknames2name_comb),\n",
    "    metric=lambda tt,pp: precision_recall_fscore_support(tt,pp, average='weighted')[0:3])\n",
    "np.mean(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../flat_data/dregs01.json\",\"r\") as fh:\n",
    "    ann_Dregs = np.asarray(json.load(fh))\n",
    "with open(\"../flat_data/dregs01.json\",\"r\") as fh:\n",
    "    ann_Dregs = np.hstack([ann_Dregs, np.asarray(json.load(fh))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_comb = np.hstack([ann_GoT, ann_Dregs])\n",
    "np.random.shuffle(ann_comb)\n",
    "                     \n",
    "scores = evaluate(ann_comb,\n",
    "                  XGBClassifier(), nicknames2name_GoT)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_characters = list(run_classifier(extract_texts_and_characters(ann_Dregs)[0], \n",
    "                       classifier=cls))\n",
    "reference_characters = extract_texts_and_characters(ann_Dregs)[1]\n",
    "print(\"acc: \", sklearn.metrics.accuracy_score(output_characters, reference_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = train_classifier(*extract_texts_and_characters(ann_GoT), \n",
    "                       classifier=XGBClassifier())\n",
    "\n",
    "output_characters = list(run_classifier(extract_texts_and_characters(ann_GoT)[0], \n",
    "                       classifier=cls,\n",
    "                       nicknames2name=nicknames2name_GoT))\n",
    "reference_characters = [datum['character'] for datum in ann_GoT]\n",
    "\n",
    "print(\"acc: \", sklearn.metrics.accuracy_score(output_characters, reference_characters))\n",
    "\n",
    "joblib.dump(cls, \"../trained_models/GoT-no-headings.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, _,vector_keys = get_feature_vectors(ann_GoT[1]['text'])\n",
    "feature_weights = list(zip(cls.feature_importances_,vector_keys))\n",
    "feature_weights.sort(reverse=True)\n",
    "feature_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = evaluate(ann_GoT, nicknames2name_GoT, XGBClassifier(n_estimators=100))\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../flat_data/Warbreaker.json\",\"r\") as fh:\n",
    "    warbreaker = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = joblib.load(\"trained_models/GoT-no-headings.pkl\")\n",
    "warbreaker_characters = run_classifier(extract_texts_and_characters(warbreaker)[0], \n",
    "                       classifier=cls,)\n",
    "ann_warbreaker = [(char, datum['text'][1:125]) for char,datum in zip(warbreaker_characters, warbreaker)]\n",
    "ann_warbreaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imp, name in zip(classifier.feature_importances_, FeatureVec().keys()):\n",
    "    print(name, \"\\t\", imp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
